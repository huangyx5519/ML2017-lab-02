{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.417258429085\n",
      "0.377930364671\n",
      "0.371048075231\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from numpy import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "#sigmoid函数\n",
    "def sigmoid(inX):\n",
    "    return 1.0/(1+exp(-inX))\n",
    "\n",
    "#只对数组索引进行随机排序\n",
    "def shuffle_data_index(data_size):\n",
    "    arr_index = []\n",
    "    for i in range(data_size):\n",
    "        arr_index.append(i)\n",
    "    random.shuffle (arr_index)\n",
    "    return arr_index\n",
    "\n",
    "#loss函数\n",
    "def loss(y,h):\n",
    "#     print(\"-(\",y,\"*\",\"log(\",h,\")+(1-\",y,\")*log(1-\",h,\")\",\"=\",-(y*log(h)+(1-y)*log(1-h)))\n",
    "    return -(y*log(h)+(1-y)*log(1-h))\n",
    "\n",
    "#验证集上测试并得到Loss函数值\n",
    "def testLogRegres(w, X_test, y_test):\n",
    "    total_cost=0\n",
    "    shuffled_arr = shuffle_data_index(y_test.size)\n",
    "    for m in range(1000):\n",
    "        index = shuffled_arr[m]\n",
    "        if(y_test[index]==-1):\n",
    "            ytr=0\n",
    "        else:\n",
    "            ytr=1\n",
    "        h = float(sigmoid(w*X_test[index].T))   \n",
    "        total_cost += loss(ytr,h)\n",
    "\n",
    "    return total_cost/(m+1)\n",
    "\n",
    "#Loss函数对w求导\n",
    "def loss_derivatived(w,X_train,y_train,index):\n",
    "    h = float(sigmoid(w*X_train[index].T))   \n",
    "    error = (h - y_train)   \n",
    "    return X_train[index]* error\n",
    "\n",
    "#数据处理\n",
    "def data_process():\n",
    "    X_train,y_train=load_svmlight_file(\"./a9a.txt\")\n",
    "    X_train=X_train.todense()  \n",
    "    t_X_row_num,t_X_column_num=shape(X_train)\n",
    "\n",
    "    #在X矩阵中添加一列“1”\n",
    "    ones_column = ones((t_X_row_num,1))\n",
    "    X_train=hstack((ones_column,X_train))\n",
    "    t_X_column_num = X_train[0].size\n",
    "    \n",
    "    \n",
    "    X_test,y_test=load_svmlight_file(\"./a9a.t\")\n",
    "    X_test=X_test.todense()  \n",
    "    t_X_row_num,t_X_column_num=shape(X_test)\n",
    "\n",
    "    #在X矩阵中添加一列“1”\n",
    "    ones_column = ones((t_X_row_num,1))\n",
    "    X_test=hstack((ones_column,X_test))\n",
    "    t_X_column_num = X_test[0].size\n",
    "    \n",
    "    #测试集补特征全零\n",
    "    zeros_row = zeros((int(X_test.size/X_test[0].size),1))\n",
    "    X_test=column_stack((X_test, zeros_row))   \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def Regression_gradAscent(X_train, X_test, y_train, y_test,alpha,maxCycles):\n",
    "    t_X_column_num=X_train[0].size\n",
    "    w = zeros((1,t_X_column_num))\n",
    "\n",
    "    p_x=[]\n",
    "    p_test_loss=[]\n",
    "\n",
    "    train_size = y_train.size\n",
    "    test_size=y_test.size\n",
    "\n",
    "    shuffled_arr = shuffle_data_index(train_size)\n",
    "    for m in range(maxCycles):\n",
    "        index = shuffled_arr[m]\n",
    "        if(y_train[index]==-1):\n",
    "            ytr=0\n",
    "        else:\n",
    "            ytr=1\n",
    "\n",
    "        w = w - alpha * loss_derivatived(w,X_train,ytr,index)\n",
    "        p_x.append(m+1)\n",
    "        p_test_loss.append(testLogRegres(w, X_test, y_test))\n",
    "\n",
    "    plt.plot(p_x, p_test_loss,label=\"default\")\n",
    "    print(p_test_loss[499])\n",
    "\n",
    "def Regression_gradAscent_NAG(X_train, X_test, y_train, y_test,alpha,maxCycles):\n",
    "    t_X_column_num=X_train[0].size\n",
    "    train_size = y_train.size\n",
    "    test_size=y_test.size\n",
    "\n",
    "    w = zeros((1,t_X_column_num))\n",
    "    v = zeros((1,t_X_column_num))\n",
    "    gamma=0.7\n",
    "\n",
    "    p_x=[]\n",
    "    p_test_loss=[]\n",
    "\n",
    "    shuffled_arr = shuffle_data_index(train_size)\n",
    "    for m in range(maxCycles):\n",
    "        index = shuffled_arr[m]\n",
    "        if(y_train[index]==-1):\n",
    "            ytr=0\n",
    "        else:\n",
    "            ytr=1\n",
    "        v=gamma*v+alpha*loss_derivatived(w-gamma*v,X_train,ytr,index)\n",
    "        w = w - v\n",
    "        p_x.append(m+1)\n",
    "        p_test_loss.append(testLogRegres(w, X_test, y_test))\n",
    "\n",
    "    plt.plot(p_x, p_test_loss,label=\"NAG\")\n",
    "    plt.legend()\n",
    "    print(p_test_loss[499])\n",
    "\n",
    "\n",
    "\n",
    "def Regression_gradAscent_Adam(X_train, X_test, y_train, y_test,alpha,maxCycles):\n",
    "    learning_rate=alpha\n",
    "    D1_decay_rate=0.9\n",
    "    D2_decay_rate=0.995\n",
    "\n",
    "\n",
    "    t_X_column_num=X_train[0].size\n",
    "    w = zeros((1,t_X_column_num))\n",
    "    s=0\n",
    "    r=0\n",
    "\n",
    "    p_x=[]\n",
    "    p_test_loss=[]\n",
    "\n",
    "    train_size = y_train.size\n",
    "    test_size=y_test.size\n",
    "\n",
    "    shuffled_arr = shuffle_data_index(train_size)\n",
    "    for m in range(maxCycles):\n",
    "        index = shuffled_arr[m]\n",
    "        if(y_train[index]==-1):\n",
    "            ytr=0\n",
    "        else:\n",
    "            ytr=1\n",
    "        dl=loss_derivatived(w,X_train,ytr,index)\n",
    "        s=(D1_decay_rate*s+(1-D1_decay_rate)*dl)\n",
    "        s1=s/(1-D1_decay_rate)\n",
    "        r=(D2_decay_rate*r+(1-D2_decay_rate)*multiply(dl,dl))\n",
    "        r1=r/(1-D2_decay_rate)\n",
    "        w=w-multiply(learning_rate*s1,1/(np.sqrt(r1)+10**-8))\n",
    "\n",
    "        p_x.append(m+1)\n",
    "        p_test_loss.append(testLogRegres(w, X_test, y_test))\n",
    "\n",
    "    plt.plot(p_x, p_test_loss,label=\"Adam\")\n",
    "    plt.legend()\n",
    "    print(p_test_loss[499])\n",
    "\n",
    "\n",
    "def Regression_gradAscent_RMSprop(X_train, X_test, y_train, y_test,alpha,maxCycles):\n",
    "    learning_rate=alpha\n",
    "    decay_rate=0.995\n",
    "\n",
    "    t_X_column_num=X_train[0].size\n",
    "    w = zeros((1,t_X_column_num))\n",
    "    r=0\n",
    "\n",
    "    p_x=[]\n",
    "    p_test_loss=[]\n",
    "\n",
    "    train_size = y_train.size\n",
    "    test_size=y_test.size\n",
    "\n",
    "    shuffled_arr = shuffle_data_index(train_size)\n",
    "    for m in range(maxCycles):\n",
    "        index = shuffled_arr[m]\n",
    "        if(y_train[index]==-1):\n",
    "            ytr=0\n",
    "        else:\n",
    "            ytr=1\n",
    "\n",
    "        dl=loss_derivatived(w,X_train,ytr,index)\n",
    "        r=decay_rate*r+(1-decay_rate)*multiply(dl,dl)\n",
    "\n",
    "        w=w-multiply(learning_rate/(np.sqrt(r)+1e-7),dl)\n",
    "\n",
    "        p_x.append(m+1)\n",
    "        p_test_loss.append(testLogRegres(w, X_test, y_test))\n",
    "\n",
    "    plt.plot(p_x, p_test_loss,label=\"RMSprop\")\n",
    "    plt.legend()\n",
    "    print(p_test_loss[499])\n",
    "\n",
    "    \n",
    "def Regression_gradAscent_Adadelta(X_train, X_test, y_train, y_test,alpha,maxCycles):\n",
    "#     learning_rate=alpha\n",
    "    decay_rate=0.9\n",
    "\n",
    "    t_X_column_num=X_train[0].size\n",
    "    w = zeros((1,t_X_column_num))\n",
    "    delta=0\n",
    "    g=0\n",
    "    e=alpha/10\n",
    "#     e=0\n",
    "\n",
    "    p_x=[]\n",
    "    p_test_loss=[]\n",
    "\n",
    "    train_size = y_train.size\n",
    "    test_size=y_test.size\n",
    "\n",
    "    shuffled_arr = shuffle_data_index(train_size)\n",
    "    for m in range(maxCycles):\n",
    "        index = shuffled_arr[m]\n",
    "        if(y_train[index]==-1):\n",
    "            ytr=0\n",
    "        else:\n",
    "            ytr=1\n",
    "\n",
    "        dl=loss_derivatived(w,X_train,ytr,index)\n",
    "        g=decay_rate*g+(1-decay_rate)*multiply(dl,dl)\n",
    "        e=decay_rate*e+(1-decay_rate)*multiply(delta,delta)\n",
    "        delta=multiply(np.sqrt(e+1e-7)/(np.sqrt(g)+1e-7),dl)\n",
    "        w=w-delta\n",
    "    \n",
    "        p_x.append(m+1)\n",
    "        p_test_loss.append(testLogRegres(w, X_test, y_test))\n",
    "\n",
    "    plt.plot(p_x, p_test_loss,label=\"Adadelta\")\n",
    "    plt.legend()\n",
    "    print(p_test_loss[499])\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    X_train, X_test, y_train, y_test=data_process()\n",
    "    alpha = 0.01\n",
    "    \n",
    "    maxCycles = 500\n",
    "    Regression_gradAscent(X_train, X_test, y_train, y_test,alpha,maxCycles)\n",
    "    Regression_gradAscent_NAG(X_train, X_test, y_train, y_test,alpha,maxCycles)\n",
    "    Regression_gradAscent_RMSprop(X_train, X_test, y_train, y_test,alpha,maxCycles)\n",
    "    Regression_gradAscent_Adam(X_train, X_test, y_train, y_test,alpha,maxCycles)\n",
    "    Regression_gradAscent_Adadelta(X_train, X_test, y_train, y_test,alpha,maxCycles)\n",
    "\n",
    "\n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
